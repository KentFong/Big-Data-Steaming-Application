{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B : Streaming Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "import pygeohash as pgh\n",
    "import pymongo\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the geohash of two locations to see if they are close to each other\n",
    "def geohash_handler(latitude, longitude, n):\n",
    "    return pgh.encode(latitude, longitude, precision=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar topic name for producers 1, 2,3\n",
    "topic_name = \"Assignment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('[Streaming from Kafka into MongoDB')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming dataframe with options providing the bootstrap server(s) and topic name.\n",
    "topic_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\n",
    "    .option('subscribe', topic_name)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient()\n",
    "# instantiating the database\n",
    "db = client.fit3182_db\n",
    "climate_collection = db.climate # collection named \"climate\"\n",
    "hotspot_collection = db.hotspot # collection names \"hotspot\"\n",
    "climate_collection.drop() # remove values inserted in Part A\n",
    "hotspot_collection.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functoin to process the batch received\n",
    "def process_batch(batch_df, batch_id):\n",
    "    aquas = [] #aqua and terra is a list, because in 10 seconds, there will be 5 instances of aqua/terra, and 1 instance of climate\n",
    "    terras = []\n",
    "    climate = {}\n",
    "    raw_data=batch_df.collect() \n",
    "    if len(raw_data) > 0: # if data exists\n",
    "        for i in range(len(raw_data)): \n",
    "            my_bytes = raw_data[i].value # values in byte array format\n",
    "            my_json = my_bytes.decode(\"utf8\").replace(\"'\", '\"') # convert to json format, using double quotes\n",
    "            data = json.loads(my_json)\n",
    "            # append based on the sender_id\n",
    "            if data[\"sender_id\"] == \"AQUA_producer\": \n",
    "                aquas.append(data)\n",
    "            elif data[\"sender_id\"] == \"TERRA_producer\":\n",
    "                terras.append(data)\n",
    "            elif data[\"sender_id\"] == \"climate_producer\":\n",
    "                climate = data\n",
    "    \n",
    "    # calculate geohash for climate with aqua\n",
    "    if climate != {}: # if climate is not None\n",
    "        hotspot = []\n",
    "        fire_event = []\n",
    "        geolat1 = climate[\"latitude\"] # calculate geohash for climate area\n",
    "        geolong1 = climate[\"longitude\"]\n",
    "        geohash1 = geohash_handler(geolat1,geolong1,3)\n",
    "        for aqua in aquas:\n",
    "            geolat2 = aqua[\"latitude\"] # calculate geohash for aqua\n",
    "            geolong2 = aqua[\"longitude\"]\n",
    "            geohash2 = geohash_handler(geolat2,geolong2,3)\n",
    "            if geohash1 == geohash2: # if similar, assign same created_date and append into hotspot list indicating that a fire has occured\n",
    "                aqua[\"created_date\"] = climate[\"created_date\"]\n",
    "                hotspot.append(aqua)\n",
    "            else:\n",
    "                # else remove from the original list\n",
    "                aquas.remove(aqua)\n",
    "        \n",
    "        # calculate geohash for climate with terra, using same procedures above\n",
    "        for terra in terras:\n",
    "            geolat3 = terra[\"latitude\"]\n",
    "            geolong3 = terra[\"longitude\"]\n",
    "            geohash3 = geohash_handler(geolat3,geolong3,3)\n",
    "            if geohash1 == geohash3:\n",
    "                terra[\"created_date\"] = climate[\"created_date\"]\n",
    "                hotspot.append(terra)\n",
    "            else:\n",
    "                terras.remove(terra)\n",
    "        \n",
    "        # if no hotspots occur for that climate data, insert and return for that batch\n",
    "        if len(hotspot) == 0:\n",
    "            climate_collection.insert_one(climate)\n",
    "            return True\n",
    "        else:\n",
    "            # if there are still aqua and terra data to compare\n",
    "            if(len(aquas)>0 and len(terras)>0):\n",
    "                # compare the geoash for aqua and terra data\n",
    "                for aqua in aquas:\n",
    "                    aqua_lat = aqua[\"latitude\"]\n",
    "                    aqua_long = aqua[\"longitude\"]\n",
    "                    hash1 = geohash_handler(aqua_lat,aqua_long,5)\n",
    "                    for terra in terras:\n",
    "                        terra_lat = terra[\"latitude\"]\n",
    "                        terra_long = terra[\"longitude\"]\n",
    "                        hash2 = geohash_handler(terra_lat,terra_long,5)\n",
    "                        # if similar, compute and update value\n",
    "                        # remove from terra, aqua, and hotspot\n",
    "                        # insert new updated hotspot value into hotspot list \n",
    "                        if hash1 == hash2:\n",
    "                            avg_hotspot = aqua\n",
    "                            avg_hotspot[\"confidence\"] = (aqua[\"confidence\"] + terra[\"confidence\"]) / 2\n",
    "                            avg_hotspot['surface_temperature_celcius'] = (\n",
    "                            aqua['surface_temperature_celcius'] + terra['surface_temperature_celcius']) / 2\n",
    "                            terras.remove(terra)\n",
    "                            hotspot.remove(terra)\n",
    "                            hotspot.remove(aqua)\n",
    "                            hotspot.append(avg_hotspot)\n",
    "                            \n",
    "            # identify the cause of fire                \n",
    "            for fire in hotspot:\n",
    "                if climate[\"air_temperature_celcius\"] > 20 and climate[\"GHI_w/m2\"] > 180:\n",
    "                    fire[\"cause\"] = \"natural\"\n",
    "                else:\n",
    "                    fire[\"cause\"] = \"other\"\n",
    "                    \n",
    "            climate_collection.insert_one(climate)\n",
    "            for x in hotspot:\n",
    "                hotspot_collection.insert_one(x)\n",
    "                \n",
    "    return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stream_df = topic_stream_df.select('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClimateWriter:\n",
    "#     # called at the start of processing each partition in each output micro-batch\n",
    "#     def open(self, partition_id, epoch_id):\n",
    "#         self.mongo_client = MongoClient(\n",
    "#             host='localhost',\n",
    "#             port=27017\n",
    "#         )\n",
    "#         self.db = self.mongo_client['fit3182_db']\n",
    "#         climate = self.db.climate\n",
    "#         return True\n",
    "    \n",
    "#     # called once per row of the result dataframe\n",
    "#     # the current code DOES NOT handle duplicate processing\n",
    "#     #   e.g., query fails and restarts just before current micro-batch was fully inserted\n",
    "#     def process(self, row):\n",
    "#         climate.insert(row)\n",
    "# #         self.db[topic_name].insert_one(row.asDict())\n",
    "\n",
    "#     def close(self, err):\n",
    "#         self.mongo_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HotspotWriter:\n",
    "#     # called at the start of processing each partition in each output micro-batch\n",
    "#     def open(self, partition_id, epoch_id):\n",
    "#         self.mongo_client = MongoClient(\n",
    "#             host='localhost',\n",
    "#             port=27017\n",
    "#         )\n",
    "#         self.db = self.mongo_client['fit3182_db']\n",
    "#         hotspot = self.db.hotspot\n",
    "#         return True\n",
    "    \n",
    "#     # called once per row of the result dataframe\n",
    "#     # the current code DOES NOT handle duplicate processing\n",
    "#     #   e.g., query fails and restarts just before current micro-batch was fully inserted\n",
    "#     def process(self, row):\n",
    "#         hotspot.insert_one(row.asDict())\n",
    "# #         self.db[topic_name].insert_one(row.asDict())\n",
    "\n",
    "#     def close(self, err):\n",
    "#         self.mongo_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_writer = (\n",
    "    output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .foreachBatch(process_batch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = db_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/student/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/student/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by CTRL-C. Stopped query\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopped query')\n",
    "except StreamingQueryException as exc:\n",
    "    print(exc)\n",
    "finally:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.fit3182_db\n",
    "climate_collection = db.climate \n",
    "m=climate_collection.find()\n",
    "for a in m:\n",
    "    pprint(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspot_collection = db.hotspot # collection names \"hotspot\"\n",
    "n = hotspot_collection.find()\n",
    "for b in n:\n",
    "    pprint(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
